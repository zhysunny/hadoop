#!/usr/bin/env bash

# 当前文件所在目录，这里是相对路径
parent=`dirname $0`
# 当前文件所在目录转为绝对路径
parent=`cd $parent;pwd`
# this为当前文件名
this=`basename $0`
# 当前文件所在目录，一般是安装路径
basedir=`cd ${parent}/../;pwd`
# 当前文件全路径
this=${parent}/${this}
# 当前文件所在目录为bin目录
bin=${parent}
if [[ -e "$basedir/conf/hadoop-config.sh" ]]; then
  # source过程中将--config和--hosts去掉了
  . ${basedir}/conf/hadoop-config.sh
else
  echo "$basedir/conf/hadoop-config.sh is not found"
  exit 1
fi

cygwin=false
case "`uname`" in
CYGWIN*) cygwin=true;;
esac

print_usage()
{
  echo "Usage: hadoop [--config confdir] COMMAND"
  echo "where COMMAND is one of:"
  echo "  namenode -format     format the DFS filesystem"
  echo "  secondarynamenode    run the DFS secondary namenode"
  echo "  namenode             run the DFS namenode"
  echo "  datanode             run a DFS datanode"
  echo "  dfsadmin             run a DFS admin client"
  echo "  mradmin              run a Map-Reduce admin client"
  echo "  fsck                 run a DFS filesystem checking utility"
  echo "  fs|dfs               run a generic filesystem user client"
  echo "  balancer             run a cluster balancing utility"
  echo "  oiv                  apply the offline fsimage viewer to an fsimage"
  echo "  fetchdt              fetch a delegation token from the NameNode"
  echo "  jobtracker           run the MapReduce job Tracker node" 
  echo "  pipes                run a Pipes job"
  echo "  tasktracker          run a MapReduce task Tracker node" 
  echo "  historyserver        run job history servers as a standalone daemon"
  echo "  job                  manipulate MapReduce jobs"
  echo "  queue                get information regarding JobQueues" 
  echo "  version              print the version"
  echo "  jar <jar>            run a jar file"
  echo "  distcp <srcurl> <desturl> copy file or directories recursively"
  echo "  distcp2 <srcurl> <desturl> DistCp version 2"
  echo "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive"
  echo "  classpath            prints the class path needed to get the"
  echo "                       Hadoop jar and the required libraries"
  echo "  daemonlog            get/set the log level for each daemon"
  echo " or"
  echo "  CLASSNAME            run the class named CLASSNAME"
  echo "Most commands print help when invoked w/o parameters."
}

# 没参数直接返回
if [[ $# = 0 ]]; then
  print_usage
  exit 1
fi

# get arguments
COMMAND=$1
shift

# support help commands
case ${COMMAND} in
  # usage flags
  --help|-help|-h|help)
    print_usage
    exit 0
    ;;
esac

# $EUID未知，$HADOOP_SECURE_DN_USER长度非0，默认未设置
if [[ "$COMMAND" == "datanode" ]] && [[ "$EUID" -eq 0 ]] && [[ -n "$HADOOP_SECURE_DN_USER" ]]; then
  HADOOP_PID_DIR=${HADOOP_SECURE_DN_PID_DIR}
  HADOOP_LOG_DIR=${HADOOP_SECURE_DN_LOG_DIR}
  HADOOP_IDENT_STRING=${HADOOP_SECURE_DN_USER}
  starting_secure_dn="true"
fi
  
if [[ "$JAVA_HOME" = "" ]]; then
  echo "Error: JAVA_HOME is not set."
  exit 1
fi

JAVA=${JAVA_HOME}/bin/java
JAVA_HEAP_MAX=-Xmx1000m 

# check envvars which might override default args
if [[ "$HADOOP_HEAPSIZE" != "" ]]; then
  #echo "run with heapsize $HADOOP_HEAPSIZE"
  JAVA_HEAP_MAX="-Xmx""$HADOOP_HEAPSIZE""m"
  #echo $JAVA_HEAP_MAX
fi

# CLASSPATH initially contains $HADOOP_CONF_DIR
CLASSPATH="${HADOOP_CONF_DIR}"
if [[ "$HADOOP_USER_CLASSPATH_FIRST" != "" ]] && [[ "$HADOOP_CLASSPATH" != "" ]] ; then
  CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}
fi
CLASSPATH=${CLASSPATH}:${JAVA_HOME}/lib/tools.jar

# for developers, add Hadoop classes to CLASSPATH
if [[ -d "$HADOOP_HOME/build/classes" ]]; then
  CLASSPATH=${CLASSPATH}:${HADOOP_HOME}/build/classes
fi
if [[ -d "$HADOOP_HOME/build/webapps" ]]; then
  CLASSPATH=${CLASSPATH}:${HADOOP_HOME}/build
fi
if [[ -d "$HADOOP_HOME/build/test/classes" ]]; then
  CLASSPATH=${CLASSPATH}:${HADOOP_HOME}/build/test/classes
fi
if [[ -d "$HADOOP_HOME/build/tools" ]]; then
  CLASSPATH=${CLASSPATH}:${HADOOP_HOME}/build/tools
fi

# so that filenames w/ spaces are handled correctly in loops below
IFS=

# for releases, add core hadoop jar & webapps to CLASSPATH
if [[ -e $HADOOP_PREFIX/share/hadoop/hadoop-core-* ]]; then
  # binary layout
  if [[ -d "$HADOOP_PREFIX/share/hadoop/webapps" ]]; then
    CLASSPATH=${CLASSPATH}:${HADOOP_PREFIX}/share/hadoop
  fi
  for f in ${HADOOP_PREFIX}/share/hadoop/hadoop-core-*.jar; do
    CLASSPATH=${CLASSPATH}:${f};
  done

  # add libs to CLASSPATH
  for f in ${HADOOP_PREFIX}/share/hadoop/lib/*.jar; do
    CLASSPATH=${CLASSPATH}:${f};
  done

  for f in ${HADOOP_PREFIX}/share/hadoop/lib/jsp-2.1/*.jar; do
    CLASSPATH=${CLASSPATH}:${f};
  done

  for f in ${HADOOP_PREFIX}/share/hadoop/hadoop-tools-*.jar; do
    TOOL_PATH=${TOOL_PATH}:${f};
  done
else
  # tarball layout
  if [[ -d "$HADOOP_HOME/webapps" ]]; then
    CLASSPATH=${CLASSPATH}:${HADOOP_HOME}
  fi
  for f in ${HADOOP_HOME}/hadoop-core-*.jar; do
    CLASSPATH=${CLASSPATH}:${f};
  done

  # add libs to CLASSPATH
  for f in ${HADOOP_HOME}/lib/*.jar; do
    CLASSPATH=${CLASSPATH}:${f};
  done

  if [[ -d "$HADOOP_HOME/build/ivy/lib/Hadoop/common" ]]; then
    for f in ${HADOOP_HOME}/build/ivy/lib/Hadoop/common/*.jar; do
      CLASSPATH=${CLASSPATH}:${f};
    done
  fi

  for f in ${HADOOP_HOME}/lib/jsp-2.1/*.jar; do
    CLASSPATH=${CLASSPATH}:${f};
  done

  for f in ${HADOOP_HOME}/hadoop-tools-*.jar; do
    TOOL_PATH=${TOOL_PATH}:${f};
  done
  for f in ${HADOOP_HOME}/build/hadoop-tools-*.jar; do
    TOOL_PATH=${TOOL_PATH}:${f};
  done
fi

# add user-specified CLASSPATH last
if [[ "$HADOOP_USER_CLASSPATH_FIRST" = "" ]] && [[ "$HADOOP_CLASSPATH" != "" ]]; then
  CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}
fi

# default log directory & file
if [[ "$HADOOP_LOG_DIR" = "" ]]; then
  HADOOP_LOG_DIR="$HADOOP_HOME/logs"
fi
if [[ "$HADOOP_LOGFILE" = "" ]]; then
  HADOOP_LOGFILE=${HADOOP_LOG_DIR}'/hadoop.log'
fi

# default policy file for service-level authorization
if [[ "$HADOOP_POLICYFILE" = "" ]]; then
  HADOOP_POLICYFILE=${HADOOP_CONF_DIR}"/hadoop-policy.xml"
fi

# restore ordinary behaviour
unset IFS

# figure out which class to run
if [[ "$COMMAND" = "classpath" ]] ; then
  if ${cygwin}; then
    CLASSPATH=`cygpath -p -w "$CLASSPATH"`
  fi
  echo ${CLASSPATH}
  exit
elif [[ "$COMMAND" = "namenode" ]] ; then
  CLASS='org.apache.hadoop.hdfs.server.namenode.NameNode'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_NAMENODE_OPTS"
elif [[ "$COMMAND" = "secondarynamenode" ]] ; then
  CLASS='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_SECONDARYNAMENODE_OPTS"
elif [[ "$COMMAND" = "datanode" ]] ; then
  CLASS='org.apache.hadoop.hdfs.server.datanode.DataNode'
  HADOOP_OPTS=${HADOOP_OPTS/-server/}
  if [[ "$starting_secure_dn" = "true" ]]; then
    HADOOP_OPTS="-jvm server $HADOOP_OPTS  $HADOOP_DATANODE_OPTS"  
  else
    HADOOP_OPTS="$HADOOP_OPTS -server $HADOOP_DATANODE_OPTS"
  fi
elif [[ "$COMMAND" = "fs" ]] ; then
  CLASS='org.apache.hadoop.fs.FsShell'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "dfs" ]] ; then
  CLASS='org.apache.hadoop.fs.FsShell'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "dfsadmin" ]] ; then
  CLASS='org.apache.hadoop.hdfs.tools.DFSAdmin'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "mradmin" ]] ; then
  CLASS='org.apache.hadoop.mapred.tools.MRAdmin'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "fsck" ]] ; then
  CLASS='org.apache.hadoop.hdfs.tools.DFSck'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "balancer" ]] ; then
  CLASS='org.apache.hadoop.hdfs.server.balancer.Balancer'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_BALANCER_OPTS"
elif [[ "$COMMAND" = "oiv" ]] ; then
  CLASS='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer'
elif [[ "$COMMAND" = "fetchdt" ]] ; then
  CLASS='org.apache.hadoop.hdfs.tools.DelegationTokenFetcher'
elif [[ "$COMMAND" = "jobtracker" ]] ; then
  CLASS='org.apache.hadoop.mapred.JobTracker'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_JOBTRACKER_OPTS"
elif [[ "$COMMAND" = "historyserver" ]] ; then
  CLASS='org.apache.hadoop.mapred.JobHistoryServer'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_JOB_HISTORYSERVER_OPTS"
elif [[ "$COMMAND" = "tasktracker" ]] ; then
  CLASS='org.apache.hadoop.mapred.TaskTracker'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_TASKTRACKER_OPTS"
elif [[ "$COMMAND" = "job" ]] ; then
  CLASS='org.apache.hadoop.mapred.JobClient'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "queue" ]] ; then
  CLASS='org.apache.hadoop.mapred.JobQueueClient'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "pipes" ]] ; then
  CLASS='org.apache.hadoop.mapred.pipes.Submitter'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "version" ]] ; then
  CLASS='org.apache.hadoop.util.VersionInfo'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "jar" ]] ; then
  CLASS='org.apache.hadoop.util.RunJar'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "distcp" ]] ; then
  CLASS='org.apache.hadoop.tools.DistCp'
  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "distcp2" ]] ; then
  CLASS='org.apache.hadoop.tools.distcp2.DistCp'
  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "daemonlog" ]] ; then
  CLASS='org.apache.hadoop.log.LogLevel'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "archive" ]] ; then
  CLASS='org.apache.hadoop.tools.HadoopArchives'
  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [[ "$COMMAND" = "sampler" ]] ; then
  CLASS='org.apache.hadoop.mapred.lib.InputSampler'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
else
  CLASS=${COMMAND}
fi

# cygwin path translation
if ${cygwin}; then
  CLASSPATH=`cygpath -p -w "$CLASSPATH"`
  HADOOP_HOME=`cygpath -w "$HADOOP_HOME"`
  HADOOP_LOG_DIR=`cygpath -w "$HADOOP_LOG_DIR"`
  TOOL_PATH=`cygpath -p -w "$TOOL_PATH"`
fi

#Determine the JAVA_PLATFORM
JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} -Xmx32m ${HADOOP_JAVA_PLATFORM_OPTS} org.apache.hadoop.util.PlatformName | sed -e "s/ /_/g"`
  
# setup 'java.library.path' for native-hadoop code if necessary
JAVA_LIBRARY_PATH=''
if [[ -d "${HADOOP_HOME}/build/native" || -d "${HADOOP_HOME}/lib/native" || -e "${HADOOP_PREFIX}/lib/libhadoop.a" ]]; then

  if [[ -d "$HADOOP_HOME/build/native" ]]; then
    JAVA_LIBRARY_PATH=${HADOOP_HOME}/build/native/${JAVA_PLATFORM}/lib
  fi
  
  if [[ -d "${HADOOP_HOME}/lib/native" ]]; then
    if [[ "x$JAVA_LIBRARY_PATH" != "x" ]]; then
      JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}
    else
      JAVA_LIBRARY_PATH=${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}
    fi
  fi

  if [[ -e "${HADOOP_PREFIX}/lib/libhadoop.a" ]]; then
    JAVA_LIBRARY_PATH=${HADOOP_PREFIX}/lib
  fi
fi

# cygwin path translation
if ${cygwin}; then
  JAVA_LIBRARY_PATH=`cygpath -p "$JAVA_LIBRARY_PATH"`
fi

HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.file=$HADOOP_LOGFILE"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.home.dir=$HADOOP_HOME"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.id.str=$HADOOP_IDENT_STRING"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}"

#turn security logger on the namenode and jobtracker only
if [[ ${COMMAND} = "namenode" ]] || [[ ${COMMAND} = "jobtracker" ]]; then
  HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,DRFAS}"
else
  HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,NullAppender}"
fi

if [[ "x$JAVA_LIBRARY_PATH" != "x" ]]; then
  HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
  export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${JAVA_LIBRARY_PATH}
fi  
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE"

# Check to see if we should start a secure datanode
if [[ "$starting_secure_dn" = "true" ]]; then
  if [[ "$HADOOP_PID_DIR" = "" ]]; then
    HADOOP_SECURE_DN_PID="/tmp/hadoop_secure_dn.pid"
  else
    HADOOP_SECURE_DN_PID="$HADOOP_PID_DIR/hadoop_secure_dn.pid"
  fi

  if [[ ${JSVC_HOME} ]]; then
    JSVC="$JSVC_HOME/jsvc"
  else
    if [[ "$JAVA_PLATFORM" = "Linux-amd64-64" ]]; then
      JSVC_ARCH="amd64"
    else
      JSVC_ARCH="i386"
    fi
    JSVC="$HADOOP_HOME/libexec/jsvc.${JSVC_ARCH}"
  fi

  if [[ ! ${JSVC_OUTFILE} ]]; then
    JSVC_OUTFILE="$HADOOP_LOG_DIR/jsvc.out"
  fi

  if [[ ! ${JSVC_ERRFILE} ]]; then
    JSVC_ERRFILE="$HADOOP_LOG_DIR/jsvc.err"
  fi

  exec "$JSVC" -Dproc_${COMMAND} -outfile "$JSVC_OUTFILE" \
               -errfile "$JSVC_ERRFILE" \
               -pidfile "$HADOOP_SECURE_DN_PID" \
               -nodetach \
               -user "$HADOOP_SECURE_DN_USER" \
               -cp "$CLASSPATH" \
               ${JAVA_HEAP_MAX} ${HADOOP_OPTS} \
               org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter "$@"
else
  # run it
  exec "$JAVA" -Dproc_${COMMAND} ${JAVA_HEAP_MAX} ${HADOOP_OPTS} -classpath "$CLASSPATH" ${CLASS} "$@"
fi
